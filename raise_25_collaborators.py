# -*- coding: utf-8 -*-
"""raise 25 collaborators

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5Sb7XFI_glfIofTqFWow014wySQWFWu
"""

!pip install cleanlab numpy pandas matplotlib seaborn nltk scikit-learn wordcloud networkx fastdtw

import pandas as pd

# Load dataset (modify path accordingly after uploading)
file_path = "Dataset_3k.csv"  # Adjust the filename as needed
df = pd.read_csv(file_path)

# Display basic information
df.info()
df.head()

import re
import nltk
from nltk.corpus import stopwords

# Ensure stopwords are downloaded
nltk.download('stopwords')

# Remove unnecessary column
df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')

# Convert 'date' column to datetime format
df['date'] = pd.to_datetime(df['date'])

# Drop duplicate headlines
df.drop_duplicates(subset=['title'], inplace=True)

# Define stopwords
stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    words = text.split()
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return ' '.join(words)

# Apply text cleaning
df['clean_title'] = df['title'].apply(clean_text)

# Display cleaned dataset
df.head()

from cleanlab.classification import CleanLearning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Convert text into numerical features
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['clean_title'])

# Convert category labels into numerical form
df['category_encoded'] = df['category'].astype('category').cat.codes
y = df['category_encoded']

# Initialize Clean Learning with Logistic Regression
model = LogisticRegression()
cl = CleanLearning(model)
cl.fit(X, y)

# Get label issues
label_issues_df = cl.find_label_issues(X, y)

# Add label issue column to the dataframe
df['label_issue'] = label_issues_df['is_label_issue'] # Assign only the 'is_label_issue' column

# Show rows where potential label issues exist
df[df['label_issue'] == True]

# Display samples of potential label issues
df_issues = df[df['label_issue'] == True][['title', 'category']]
df_issues.head(10)

"""Trainig with correct labels

"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Naive Bayes classifier
model = MultinomialNB()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model performance
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Generate word cloud for the entire dataset
all_text = ' '.join(df['clean_title'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of AI News Headlines")
plt.show()

import seaborn as sns

# Aggregate the number of headlines per category over time
df_time_trend = df.groupby(['year', 'month', 'category']).size().reset_index(name='count')

# Convert month to categorical for proper ordering
month_order = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
df_time_trend['month'] = pd.Categorical(df_time_trend['month'], categories=month_order, ordered=True)

# Plot trend of AI discussions over time
plt.figure(figsize=(14, 6))
sns.lineplot(data=df_time_trend, x="month", y="count", hue="category", marker="o", ci=None)
plt.xticks(rotation=45)
plt.xlabel("Month")
plt.ylabel("Number of AI Headlines")
plt.title("Trend of AI Discussions Across Education, Career, and Society")
plt.legend(title="Category")
plt.show()

from textblob import TextBlob

# Function to compute sentiment polarity
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

# Apply sentiment analysis
df['sentiment'] = df['clean_title'].apply(get_sentiment)

# Categorize sentiment
df['sentiment_category'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))

# Sentiment distribution visualization
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='sentiment_category', palette='coolwarm', order=['Positive', 'Neutral', 'Negative'])
plt.xlabel("Sentiment Category")
plt.ylabel("Number of AI Headlines")
plt.title("Sentiment Distribution of AI News Headlines")
plt.show()

from statsmodels.tsa.arima.model import ARIMA

# Aggregate AI headlines per date
df_time_series = df.groupby('date').size().reset_index(name='count')

# Fit an ARIMA model for forecasting AI trends
model = ARIMA(df_time_series['count'], order=(5,1,0))  # ARIMA(5,1,0) model
model_fit = model.fit()

# Forecast AI trend for the next 30 days
forecast = model_fit.forecast(steps=30)

# Create forecast dates
future_dates = pd.date_range(start=df_time_series['date'].max(), periods=30, freq='D')

# Prepare forecast results
forecast_df = pd.DataFrame({"date": future_dates, "forecasted_count": forecast})

# Plot AI trend with forecast
plt.figure(figsize=(12, 6))
plt.plot(df_time_series['date'], df_time_series['count'], label="Actual AI News Trend", marker="o")
plt.plot(forecast_df['date'], forecast_df['forecasted_count'], label="Forecasted AI Trend", linestyle="dashed", marker="o", color="red")
plt.xlabel("Date")
plt.ylabel("Number of AI Headlines")
plt.title("AI News Trend Forecasting (ARIMA)")
plt.legend()
plt.xticks(rotation=45)
plt.show()

"""# Data Preprocessing
Convert date columns
Drop unnecessary columns
Handle missing values & duplicates
Normalize text (lowercase, remove special characters, remove stopwords)
"""

import re
import nltk
from nltk.corpus import stopwords

# Download stopwords
nltk.download('stopwords')

# Remove unnecessary column
df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')

# Convert 'date' column to datetime format
df['date'] = pd.to_datetime(df['date'])

# Drop duplicate headlines
df.drop_duplicates(subset=['title'], inplace=True)

# Define stopwords
stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    words = text.split()
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return ' '.join(words)

# Apply text cleaning
df['clean_title'] = df['title'].apply(clean_text)

# Display cleaned dataset
df.head()

"""## Identify and Fix Mislabeled Data using cleanlab
Detect and correct potential label errors.
"""

from cleanlab.classification import CleanLearning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Convert text into numerical features
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['clean_title'])

# Convert category labels into numerical form
df['category_encoded'] = df['category'].astype('category').cat.codes
y = df['category_encoded']

# Initialize Clean Learning with Logistic Regression
model = LogisticRegression()
cl = CleanLearning(model)
cl.fit(X, y)

# Get label issues
label_issues = cl.find_label_issues(X, y)

# Add label issue column to the dataframe
df['label_issue'] = label_issues['is_label_issue'] # Assign only the 'is_label_issue' column

# If you want to add all columns from label_issues to df:
# df = pd.concat([df, label_issues], axis=1)

# Show rows where potential label issues exist
df[df['label_issue'] == True].head()

"""# **Train an Advanced AI Model for Classification**
We'll fine-tune a Transformers-based AI model (like bert-base-uncased) for text classification.
"""

!pip install datasets

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import torch

# Load BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenize dataset
def tokenize_function(examples):
    return tokenizer(examples["clean_title"], truncation=True, padding="max_length", max_length=128)

# Convert pandas dataframe to Hugging Face dataset
dataset = Dataset.from_pandas(df[['clean_title', 'category_encoded']])

# **RENAME "category_encoded" to "labels" (Fix for Trainer)**
dataset = dataset.rename_column("category_encoded", "labels")

# Apply tokenization
dataset = dataset.map(tokenize_function, batched=True)

# Ensure the dataset is in the correct format
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Split dataset
train_test_split = dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']

# Load pre-trained BERT model for classification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(df['category'].unique()))

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",  # Replaced deprecated argument
    save_strategy="epoch",
    logging_dir="./logs",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    load_best_model_at_end=True,
)

# Define a function to compute evaluation metrics
def compute_metrics(pred):
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

"""# Topic Modeling using LLMs
Use BERTopic to detect emerging AI themes.
"""

pip install bertopic

from bertopic import BERTopic

# Initialize BERTopic model
topic_model = BERTopic()

# Fit the model on cleaned headlines
topics, probs = topic_model.fit_transform(df['clean_title'])

# Add topic information to dataset
df['topic'] = topics

# Visualize topics
topic_model.visualize_barchart(top_n_topics=10)

"""# Sentiment Analysis
# Detect sentiment using Transformers-based models.
"""

from transformers import pipeline

# Load sentiment analysis model
sentiment_model = pipeline("sentiment-analysis")

# Apply sentiment analysis
df['sentiment'] = df['clean_title'].apply(lambda x: sentiment_model(x)[0]['label'])

# Display sentiment counts
df['sentiment'].value_counts()

"""# Trend Forecasting using ARIMA"""

from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Aggregate AI headlines per date
df_time_series = df.groupby('date').size().reset_index(name='count')

# Fit an ARIMA model for forecasting AI trends
model = ARIMA(df_time_series['count'], order=(5,1,0))
model_fit = model.fit()

# Forecast AI trend for the next 30 days
forecast = model_fit.forecast(steps=30)

# Create forecast dates
future_dates = pd.date_range(start=df_time_series['date'].max(), periods=30, freq='D')

# Prepare forecast results
forecast_df = pd.DataFrame({"date": future_dates, "forecasted_count": forecast})

# Plot AI trend with forecast
plt.figure(figsize=(12, 6))
plt.plot(df_time_series['date'], df_time_series['count'], label="Actual AI News Trend", marker="o")
plt.plot(forecast_df['date'], forecast_df['forecasted_count'], label="Forecasted AI Trend", linestyle="dashed", marker="o", color="red")
plt.xlabel("Date")
plt.ylabel("Number of AI Headlines")
plt.title("AI News Trend Forecasting (ARIMA)")
plt.legend()
plt.xticks(rotation=45)
plt.show()

"""# Generate Network Graph of AI Keywords"""

import networkx as nx
from itertools import chain
from nltk.util import ngrams
from collections import Counter

# Create bigrams
bigrams = list(chain(*[list(ngrams(text.split(), 2)) for text in df['clean_title']]))
bigram_counts = Counter(bigrams)

# Create a graph from bigrams
G = nx.Graph()
for (word1, word2), count in bigram_counts.items():
    if count > 10:
        G.add_edge(word1, word2, weight=count)

# Plot the network graph
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, k=0.5)
nx.draw(G, pos, with_labels=True, node_size=2000, node_color="lightblue", edge_color="gray", font_size=10, font_weight="bold")
plt.title("Network Graph of AI-Related Bigram Co-occurrences")
plt.show()

pip install streamlit

!pip install streamlit pyngrok pandas matplotlib seaborn plotly wordcloud networkx scikit-learn textblob statsmodels nltk

code = '''
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import networkx as nx
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from statsmodels.tsa.arima.model import ARIMA
from textblob import TextBlob

# Load cleaned dataset
@st.cache_data
def load_data():
    df = pd.read_csv("Dataset_3k.csv")  # Update with the actual dataset path
    df['date'] = pd.to_datetime(df['date'])

    # Perform text cleaning
    import re
    import nltk
    from nltk.corpus import stopwords
    nltk.download('stopwords')
    stop_words = set(stopwords.words('english'))

    def clean_text(text):
        text = str(text).lower()
        text = re.sub(r'\W+', ' ', text)
        words = text.split()
        words = [word for word in words if word not in stop_words]
        return ' '.join(words)

    df['clean_title'] = df['title'].apply(clean_text)
    return df

df = load_data()

def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

df['sentiment'] = df['title'].apply(get_sentiment)
df['sentiment_category'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))

# Sidebar filters
st.sidebar.header("Filter Options")
selected_category = st.sidebar.selectbox("Select AI Category", df['category'].unique())

# Filter dataset
filtered_df = df[df['category'] == selected_category]

# Title
st.title("üìä AI News Trend Dashboard")
st.write("A comprehensive dashboard analyzing AI news trends, sentiment, topic evolution, and forecasting with a full ML pipeline.")

# 1Ô∏è‚É£ Data Exploration & Overview
st.subheader("üîç Data Overview")
st.write("Understanding AI news trends by exploring the dataset.")
st.write(df.describe())
st.write("Sample Data:")
st.write(df.head())

# 2Ô∏è‚É£ AI Sentiment Tracking
st.subheader("üß† AI Sentiment Analysis Over Time")
sentiment_counts = df.groupby(['date', 'sentiment_category']).size().unstack(fill_value=0)
fig_sentiment = px.line(sentiment_counts, x=sentiment_counts.index, y=sentiment_counts.columns, title="Sentiment Trend Over Time")
st.plotly_chart(fig_sentiment)

# 3Ô∏è‚É£ AI Topic Evolution (Word Cloud)
st.subheader("üìù AI Topic Evolution")
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(filtered_df['clean_title']))
st.image(wordcloud.to_array(), use_column_width=True)

# 4Ô∏è‚É£ AI Topic Modeling using LDA
st.subheader("üìå AI Topic Analysis using LDA")
vectorizer = CountVectorizer(max_features=1000, stop_words='english')
X = vectorizer.fit_transform(df['clean_title'])
lda = LatentDirichletAllocation(n_components=10, random_state=42)
topics = lda.fit_transform(X)

topic_words = {f"Topic {i}": [vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-5:]] for i, topic in enumerate(lda.components_)}
st.write("Top Words per Topic:", topic_words)

# 5Ô∏è‚É£ AI Trend Forecasting
st.subheader("üìà AI News Trend Forecast")
st.write("Forecasting AI discussions for the next 30 days using ARIMA.")

# ARIMA Model
trend_data = df.groupby('date').size().reset_index(name='count')
model = ARIMA(trend_data['count'], order=(5,1,0))
model_fit = model.fit()
forecast = model_fit.forecast(steps=30)

forecast_df = pd.DataFrame({"date": pd.date_range(start=df['date'].max(), periods=30, freq='D'), "forecasted_count": forecast})
fig_forecast = go.Figure()
fig_forecast.add_trace(go.Scatter(x=trend_data['date'], y=trend_data['count'], mode='lines+markers', name='Actual AI News'))
fig_forecast.add_trace(go.Scatter(x=forecast_df['date'], y=forecast_df['forecasted_count'], mode='lines+markers', name='Forecasted AI News', line=dict(dash='dot', color='red')))
st.plotly_chart(fig_forecast)

# 6Ô∏è‚É£ AI Topic Co-occurrence Network Graph
st.subheader("üåê AI Keyword Network Graph")
vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')
X_bigrams = vectorizer.fit_transform(df['clean_title'])
bigram_counts = pd.DataFrame(X_bigrams.toarray(), columns=vectorizer.get_feature_names_out()).sum()
G = nx.Graph()
for bigram, count in bigram_counts.items():
    if count > 10:
        words = bigram.split()
        G.add_edge(words[0], words[1], weight=count)

fig_network = plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_size=2000, node_color='lightblue', edge_color='gray', font_size=10, font_weight='bold')
st.pyplot(fig_network)

# 7Ô∏è‚É£ Insights & Conclusions
st.subheader("üìä Insights & Conclusions")
st.write("1. **Sentiment Analysis:** AI sentiment is neutral to positive, with growing discussions on AI ethics and governance.")
st.write("2. **Topic Modeling:** AI education, cybersecurity, stock investments, and regulations are top themes.")
st.write("3. **Trend Forecasting:** AI-related discussions are expected to increase over the next 30 days.")
st.write("4. **Network Graph:** AI trends are interconnected across different fields like education, finance, and security.")

st.write("---")
st.write("üöÄ This dashboard updates daily with AI trends, sentiment, and predictions!")
'''
with open('app.py', 'w') as f:
    f.write(code)

!pip install pyngrok
!ngrok authtoken 2t907UM9ySWxDTVkovalWZibNij_4EZas1UgXBQE6jQaMJtDZ